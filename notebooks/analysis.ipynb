{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sae\n",
    "\n",
    "model: `sae.ep1.pt`\n",
    "\n",
    "* model: recurrentgemma-9b (d_model=4,096, layer=30, activation=rg_lru)\n",
    "* Dataset: minipile (110K data instances = 1/10 of the entire dataset)\n",
    "* num sae feat: 131K (32x d_model)\n",
    "* sae type: topk, k=32\n",
    "* training tokens: 259M (1 ep.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tg352/.conda/envs/python311/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import jaxtyping as jt\n",
    "import kagglehub\n",
    "from pathlib import Path\n",
    "from openai_sparse_autoencoder.train import FastAutoencoder\n",
    "from typing import List\n",
    "import sentencepiece as spm\n",
    "import gzip\n",
    "import random\n",
    "\n",
    "Fl = lambda size: jt.Float[torch.Tensor, size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/share/rush/tg352/sae/minipile/9b/artefacts/\"\n",
    "MODEL_PATH = \"/home/tg352/sparse_autoencoder/artefacts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_dir = Path(\n",
    "    kagglehub.model_download(f\"google/recurrentgemma/pyTorch/9b\")\n",
    ")\n",
    "vocab_path = weights_dir / \"tokenizer.model\"\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.Load(str(vocab_path))\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sae.ep1.pt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae_model_pt = \"sae.ep1.pt\"\n",
    "sae_model_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tg352/.conda/envs/python311/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FastAutoencoder(\n",
       "  (encoder): Linear(in_features=4096, out_features=131072, bias=False)\n",
       "  (decoder): Linear(in_features=131072, out_features=4096, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 4_096\n",
    "sae = FastAutoencoder(\n",
    "    n_dirs_local=d_model * 32,\n",
    "    d_model=d_model,\n",
    "    k=32,\n",
    "    dead_steps_threshold=None,\n",
    "    auxk=None,\n",
    ")\n",
    "sae.eval()\n",
    "sae.load_state_dict(\n",
    "    torch.load(\n",
    "        os.path.join(MODEL_PATH, sae_model_pt),\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "        weights_only=True,\n",
    "    )\n",
    ")\n",
    "sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frac-dead: tensor(0.0149)\n",
      "total: 131072, dead = 1958\n"
     ]
    }
   ],
   "source": [
    "print(\"frac-dead: \", end=\"\")\n",
    "print(torch.sum(sae.stats_last_nonzero > (10_000_000 // 16384)) / (32 * 4_096))\n",
    "print(f\"total: {32 * 4_096}, dead = {torch.sum(sae.stats_last_nonzero > (10_000_000 // 16384))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pid.0-batch.37491.pkl.gz'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_filenames = os.listdir(DATA_PATH)\n",
    "act_filename = random.choice(act_filenames)\n",
    "act_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1346, torch.Size([1346, 4096]), torch.float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_acts(filename: str) -> [list[str], Fl(\"l d\")]:\n",
    "    act_dict = pickle.load(gzip.open(os.path.join(DATA_PATH, filename), \"rb\"))\n",
    "    tokens = [vocab.IdToPiece(input_id) for input_id in act_dict[\"input_ids\"][0].tolist()]\n",
    "    return tokens, act_dict[f\"blocks.30\"][\"rg_lru_states\"][0].float()\n",
    "\n",
    "\n",
    "toks, acts = get_acts(act_filename)\n",
    "len(toks), acts.shape, acts.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(18.8750), tensor(-20.1250))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acts.max(), acts.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1346, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_feats(acts: Fl(\"l d\"), k: int = 1) -> [Fl(\"l k\"), Fl(\"l k\")]:\n",
    "    return sae.encode(acts, k=k)\n",
    "    \n",
    "\n",
    "feat_idxs, feat_vals = get_feats(acts)\n",
    "feat_idxs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1346, 1]),\n",
       " tensor([[97060],\n",
       "         [97060],\n",
       "         [97060],\n",
       "         ...,\n",
       "         [93799],\n",
       "         [77678],\n",
       "         [77678]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_vals.shape, feat_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;178mtok\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def highlight_tok(tok: str, act_val: float) -> str:\n",
    "    if act_val < 0:\n",
    "        # Red spectrum (160-196 in 256-color mode).\n",
    "        intensity = int(160 + (1 + act_val) * 36)\n",
    "    else:\n",
    "        # Blue spectrum (34-46 in 256-color mode).\n",
    "        intensity = int(34 + act_val * 12)\n",
    "    return f\"\\033[38;5;{intensity}m{tok}\\033[0m\"\n",
    "\n",
    "print(highlight_tok(\"tok\", -0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_tok(tok: str, act_val: float) -> str:\n",
    "    if act_val >= 0:\n",
    "        return f\"\\033[32m{tok}\\033[0m\"\n",
    "    else:\n",
    "        return f\"\\033[31m{tok}\\033[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_feat(feat_idx: int, max_num_files: int = 10, min_tok_len: int | None = None):\n",
    "    num_files_w_feat = 0\n",
    "    for filename in act_filenames:\n",
    "        if num_files_w_feat > max_num_files:\n",
    "            break\n",
    "        \n",
    "        toks, acts = get_acts(filename)\n",
    "        if min_tok_len is not None and len(toks) < min_tok_len:\n",
    "            continue\n",
    "        topk_idxs, topk_vals = get_feats(acts, k=1)  # (seq_len, k), (seq_len, k)\n",
    "        # print(topk_idxs)\n",
    "        if feat_idx not in topk_idxs:\n",
    "            continue\n",
    "        \n",
    "        mask = (topk_idxs == feat_idx).squeeze(1)\n",
    "        text_to_print = []\n",
    "        for tok, val, is_feat in zip(toks, topk_vals.squeeze(1), mask):\n",
    "            if is_feat:\n",
    "                text_to_print.append(f\"{highlight_tok(tok, val)}\")\n",
    "            else:\n",
    "                text_to_print.append(tok)\n",
    "        print(f\"{filename}\\n{'-' * len(filename)}\")\n",
    "        print(\"\".join(text_to_print))\n",
    "        print(\"\\n\\n\")\n",
    "        \n",
    "        num_files_w_feat += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feat(103, max_num_files=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
